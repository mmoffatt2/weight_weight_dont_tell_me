# mostly generated by kimi k2
from transformers.dynamic_module_utils import resolve_trust_remote_code
import torch
import torch.nn as nn
from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM
from transformers import OlmoeForCausalLM
import gc
import os
import json


class OLMoEModifierMemoryEfficient:
    def __init__(self, model_name="allenai/OLMoE-1B-7B-0924", expert_to_remove=0):
        self.model_name = model_name
        self.expert_to_remove = expert_to_remove
        self.config = None
        self.device = None

    def setup_device(self):
        """Setup MPS device with fallback to CPU"""
        if torch.backends.mps.is_available():
            self.device = torch.device("mps")
            print(f"‚úÖ MPS device available: {self.device}")
        else:
            self.device = torch.device("cpu")
            print("‚ö†Ô∏è MPS not available, using CPU")
        return self.device

    def remove_accelerate_hooks(self, module):
        """Recursively remove accelerate hooks from a module"""
        if hasattr(module, "_hf_hook"):
            delattr(module, "_hf_hook")
        for child in module.children():
            self.remove_accelerate_hooks(child)

    def modify_and_save_layer_by_layer(self, output_dir, torch_dtype=torch.float16):
        """
        Memory-efficient: Load, modify, and save each layer individually
        Uses ~2-3GB RAM instead of full model size
        """
        print("\nüîß Starting layer-by-layer modification...")

        # Load config
        self.config = AutoConfig.from_pretrained(self.model_name)
        original_num_experts = self.config.num_experts

        # Update config
        self.config.num_experts = original_num_experts - 1

        # Create output directory
        os.makedirs(output_dir, exist_ok=True)

        # Load tokenizer
        tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        tokenizer.save_pretrained(output_dir)

        # Load the model with device_map for minimal memory usage
        print("Loading model with device_map for minimal memory...")

        # I need to not do this
        model = OlmoeForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch_dtype,
            device_map=None,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
        )

        # Process each layer
        print(f"\nProcessing {len(model.model.layers)} layers...")

        for layer_idx, layer in enumerate(model.model.layers):
            print(f"\nüì¶ Layer {layer_idx + 1}/{len(model.model.layers)}")

            # Move layer to device for modification
            layer = layer.to(self.device)

            # Modify the MoE layer
            if hasattr(layer, "mlp") and hasattr(layer.mlp, "experts"):
                mlp = layer.mlp

                # Remove expert
                num_experts = len(mlp.experts)
                new_experts = nn.ModuleList()
                for i in range(num_experts):
                    if i != self.expert_to_remove:
                        new_experts.append(mlp.experts[i])
                mlp.experts = new_experts

                # Adjust router
                if hasattr(mlp, "gate"):
                    self._adjust_router(mlp.gate, num_experts, num_experts - 1)

            # Move layer back to CPU to free memory
            layer = layer.to("cpu")

            # Clear cache
            if self.device.type == "mps":
                torch.mps.empty_cache()
            else:
                gc.collect()

        # CRITICAL: Remove hooks before saving
        print("\nü™ù Removing accelerate hooks...")
        self.remove_accelerate_hooks(model)

        # Save the modified model
        print("\nüíæ Saving modified model...")
        model.save_pretrained(output_dir)

        # Save updated config(overwrite old one)
        self.config.save_pretrained(output_dir)

        print("‚úÖ Layer-by-layer modification complete!")
        return model

    def _adjust_router(self, router, old_num_experts, new_num_experts):
        """Adjust router layer dimensions (same as before)"""
        print(f"  Adjusting router: {router}")

        original_weight = router.weight.data
        original_bias = router.bias.data if router.bias is not None else None

        # Create new weight matrix
        new_weight = torch.zeros(
            new_num_experts,
            router.in_features,
            dtype=original_weight.dtype,
            device=original_weight.device,
        )

        # Copy weights for remaining experts
        new_idx = 0
        for i in range(old_num_experts):
            if i != self.expert_to_remove:
                new_weight[new_idx] = original_weight[i]
                new_idx += 1

        # Update router
        router.out_features = new_num_experts
        router.weight = nn.Parameter(new_weight)

        # Adjust bias
        if original_bias is not None:
            new_bias = torch.zeros(
                new_num_experts, dtype=original_bias.dtype, device=original_bias.device
            )
            new_idx = 0
            for i in range(old_num_experts):
                if i != self.expert_to_remove:
                    new_bias[new_idx] = original_bias[i]
                    new_idx += 1
            router.bias = nn.Parameter(new_bias)

        print(f"  Router adjusted from {old_num_experts} to {new_num_experts} outputs")


def main_memory_efficient():
    """Example using memory-efficient layer-by-layer modification"""
    MODEL_NAME = "allenai/OLMoE-1B-7B-0924"
    EXPERT_TO_REMOVE = 0
    OUTPUT_DIR = "./olmoe_modified_efficient"

    print("=" * 60)
    print("OLMoE Expert Removal - Memory Efficient Mode")
    print("=" * 60)

    # # need to clean up
    modifier = OLMoEModifierMemoryEfficient(
        model_name=MODEL_NAME, expert_to_remove=EXPERT_TO_REMOVE
    )
    #
    # # Setup device
    device = modifier.setup_device()
    #
    # # Modify and save layer by layer
    model = modifier.modify_and_save_layer_by_layer(
        output_dir=OUTPUT_DIR, torch_dtype=torch.float16
    )

    # Test the current model
    print("\nüß™ Testing modified model...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)

        config = AutoConfig.from_pretrained(OUTPUT_DIR)

        # Load the saved model for testing
        test_model = AutoModelForCausalLM.from_pretrained(
            OUTPUT_DIR, device_map=None, dtype=torch.float16
        )

        test_prompt = "The future of AI is"
        # inputs = tokenizer(test_prompt, return_tensors="pt").to(device)
        inputs = tokenizer(test_prompt, return_tensors="pt")

        with torch.no_grad():
            outputs = test_model.generate(
                **inputs,
                max_new_tokens=30,
                do_sample=True,
                temperature=0.7,
                pad_token_id=tokenizer.eos_token_id,
            )

        print(f"Output: {tokenizer.decode(outputs[0], skip_special_tokens=True)}")
        print("\n‚úÖ Test successful!")

    except Exception as e:
        print(f"‚ö†Ô∏è Test failed: {e}")

    # Cleanup
    print("\nüßπ Cleaning up...")
    # del model, test_model
    # if device.type == "mps":
    #     torch.mps.empty_cache()
    # else:
    #     gc.collect()

    print("\nüéâ Done! Modified model saved to:", OUTPUT_DIR)


if __name__ == "__main__":
    main_memory_efficient()
