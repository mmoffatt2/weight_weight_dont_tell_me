{
  "mistralai/Mixtral-8x7B-v0.1": {
    "router_key": "router_logits",
    "num_hidden_layers": 32,
    "num_routed_experts": 8,
    "outputs_logits": true,
    "num_experts_per_token": 2,
    "forward_kwargs": { "output_router_logits": true }
  },
  "mistralai/Mixtral-8x7B-Instruct-v0.1": {
    "router_key": "router_logits",
    "num_hidden_layers": 32,
    "num_routed_experts": 8,
    "outputs_logits": true,
    "num_experts_per_token": 2,
    "forward_kwargs": { "output_router_logits": true }
  },
  "deepseek-ai/DeepSeek-V2-Lite": {
  "router_key": "router_probs",
  "num_hidden_layers": 27,
  "num_routed_experts": 64,
  "outputs_logits": false,
  "num_experts_per_token": 6
  },
  "deepseek-ai/DeepSeek-V2": {
    "router_key": "router_probs",
    "num_hidden_layers": 60,
    "num_routed_experts": 64,
    "outputs_logits": false,
    "num_experts_per_token": 6
  },
  "deepseek-ai/deepseek-moe-16b-base": {
    "router_key": null,
    "num_hidden_layers": 28,
    "num_routed_experts": 64,
    "num_experts_per_token": 6,
    "outputs_logits": false,
    "forward_kwargs": {"use_cache": false}
  },
  "Qwen/Qwen1.5-MoE-A2.7B": {
    "router_key": "moe_router_logits",
    "num_hidden_layers": 24,
    "num_routed_experts": 16,
    "outputs_logits": true,
    "num_experts_per_token": 2
  },
  "Qwen/Qwen2-MoE-A14B": {
    "router_key": "router_logits",
    "num_hidden_layers": 40,
    "num_routed_experts": 60,
    "outputs_logits": true,
    "num_experts_per_token": 2
  },
  "microsoft/Phi-3.5-MoE-instruct": {
    "router_key": "router_probs",
    "num_hidden_layers": 24,
    "num_routed_experts": 8,
    "outputs_logits": false,
    "num_experts_per_token": 2
  },
  "Yi-1.5-MoE-9B": {
    "router_key": "router_logits",
    "num_hidden_layers": 32,
    "num_routed_experts": 16,
    "outputs_logits": true,
    "num_experts_per_token": 2
  }
}
